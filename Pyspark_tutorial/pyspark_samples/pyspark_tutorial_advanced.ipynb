{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a6a10e",
   "metadata": {},
   "source": [
    "# PySpark Tutorial — Beginner to Advanced\n",
    "Comprehensive notes, theory (Hadoop/HDFS/RDD/MapReduce), practical PySpark examples, advanced topics, diagrams, and sample datasets.\n",
    "Each code cell includes comments explaining what it does, internal working notes, and keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1d6f7",
   "metadata": {},
   "source": [
    "## Setup: install & environment\n",
    "**What this cell does:** shows how to install PySpark and required libs. **Note:** Running PySpark requires Java and Spark binaries when creating a real SparkSession; code here is for the notebook — uncomment install commands if needed.\n",
    "\n",
    "**Keywords:** `pyspark`, `SparkSession`, `spark-submit`, `PYSPARK_PYTHON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark (uncomment to run)\n",
    "# !pip install pyspark\n",
    "# !pip install findspark  # optional helper\n",
    "\n",
    "# Typical SparkSession creation (do not run unless Java/Spark present)\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName('pyspark-notebook') \\\n",
    "#     .config('spark.some.config.option', 'some-value') \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad2485",
   "metadata": {},
   "source": [
    "## Big Data Theory: Hadoop, HDFS, MapReduce, Distributed Systems (Short)\n",
    "- **Hadoop**: ecosystem for distributed storage and processing.\n",
    "- **HDFS**: distributed file system with NameNode (metadata) and DataNodes (blocks). Stores files split into blocks (default 128MB) replicated across DataNodes.\n",
    "- **MapReduce**: programming model (Map + Shuffle + Reduce). Batch processing; writes intermediate results to disk.\n",
    "- **Distributed Systems basics**: partitioning, replication, consensus, fault tolerance, data locality (run close to data), network and disk IO bottlenecks.\n",
    "\n",
    "**Keywords:** blocks, replication, NameNode, DataNode, mapper, reducer, shuffle, data locality, eventual consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4bd10",
   "metadata": {},
   "source": [
    "## RDDs (Resilient Distributed Datasets)\n",
    "**What:** low-level immutable distributed collection of objects. Operations: transformations (map, filter) and actions (collect, count).\n",
    "**Internals:** lineage graph for fault tolerance; recompute partitions on failure; operations are lazy; partitions are units of parallelism.\n",
    "\n",
    "**Keywords:** partition, lineage, transformation, action, narrow/wide dependencies, shuffle, checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example RDD code (conceptual, inside SparkSession)\n",
    "# rdd = spark.sparkContext.parallelize([1,2,3,4], numSlices=2)\n",
    "# rdd2 = rdd.map(lambda x: x*2)\n",
    "# print(rdd2.collect())\n",
    "\n",
    "# Notes: avoid collect() on large datasets; use actions like count(), take(n) for sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde1640",
   "metadata": {},
   "source": [
    "## Spark Core: Transformations, Actions, DAG, Stages & Tasks\n",
    "- **Transformations**: lazy operations returning new RDD/DataFrame (map, filter, select, join).\n",
    "- **Actions**: trigger execution (count, collect, save).\n",
    "- **DAG**: optimizer builds Directed Acyclic Graph of stages.\n",
    "- **Stages**: split by shuffle boundaries; each stage contains tasks executed in parallel per partition.\n",
    "\n",
    "**Keywords:** lineage, shuffle, stage, task, narrow dependency, wide dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb9274",
   "metadata": {},
   "source": [
    "## PySpark DataFrame API — Basics\n",
    "**What this cell does:** introduces creation and basic operations on DataFrames.\n",
    "**Internals:** PySpark DataFrame is a distributed collection of rows with schema; uses Catalyst optimizer and Tungsten execution backend.\n",
    "**Keywords:** SparkSession, DataFrame, Column, select, filter, withColumn, groupBy, agg, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example PySpark DataFrame usage (conceptual code to be run in a Spark environment)\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName('demo').getOrCreate()\n",
    "# df = spark.read.csv('Pyspark_tutorial/pyspark_samples/sales.csv', header=True, inferSchema=True)\n",
    "# df.printSchema()\n",
    "# df.show(5)\n",
    "# df.select('customer', 'amount').filter(df.amount > 100).show()\n",
    "\n",
    "# Note: the above commands work in an environment with Spark installed and Java available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0daa85b",
   "metadata": {},
   "source": [
    "## Reading & Writing Data\n",
    "**What this cell does:** demonstrates formats and options.\n",
    "**Internals:** Spark supports many data sources (CSV, Parquet, ORC, JSON, JDBC). Parquet + columnar formats are preferred for performance.\n",
    "**Keywords:** parquet, parquet predicate pushdown, partitionBy, mode, header, inferSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08711e82",
   "metadata": {},
   "source": [
    "## Partitioning, Shuffle, & Broadcast Joins\n",
    "**What this cell does:** explains how data movement happens and best practices.\n",
    "**Internals:** Shuffle writes intermediate data across network and disk; expensive. Broadcast join sends small table to all executors to avoid shuffle.\n",
    "**Keywords:** repartition, coalesce, broadcast, shuffle, shuffle partitions (spark.sql.shuffle.partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41021931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast join example (conceptual)\n",
    "# from pyspark.sql.functions import broadcast\n",
    "# small = spark.read.csv('/small.csv', header=True, inferSchema=True)\n",
    "# large = spark.read.csv('/large.csv', header=True, inferSchema=True)\n",
    "# joined = large.join(broadcast(small), on='id')\n",
    "# joined.show()\n",
    "\n",
    "# Repartition vs coalesce notes:\n",
    "# - repartition(n): full shuffle to create n partitions\n",
    "# - coalesce(n): avoid full shuffle when decreasing partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f83c71",
   "metadata": {},
   "source": [
    "## Checkpointing & Fault Tolerance\n",
    "**What this cell does:** describes checkpointing and lineage management.\n",
    "**Internals:** RDD/DataFrame lineage records transformations; excessive lineage increases job planning overhead; checkpointing writes to reliable storage to truncate lineage.\n",
    "**Keywords:** checkpoint, setCheckpointDir, lineage, fault recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd14de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint example (conceptual)\n",
    "# spark.sparkContext.setCheckpointDir('/tmp/checkpoints')\n",
    "# rdd = spark.sparkContext.parallelize(range(1000)).map(lambda x: x+1)\n",
    "# rdd_checkpointed = rdd.checkpoint()\n",
    "# rdd_checkpointed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7079e",
   "metadata": {},
   "source": [
    "## Structured Streaming (Intro)\n",
    "**What this cell does:** introduces streaming model and concepts.\n",
    "**Internals:** Structured Streaming treats streams as unbounded tables; uses micro-batch or continuous processing; supports event-time aggregations and watermarks.\n",
    "**Keywords:** readStream, writeStream, trigger, watermark, window, outputMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Streaming example (conceptual)\n",
    "# df_stream = spark.readStream.option('sep',',').csv('/stream/path')\n",
    "# query = (df_stream\n",
    "#          .withColumn('event_time', to_timestamp(df_stream.ts))\n",
    "#          .groupBy(window(df_stream.event_time, '1 minute'), df_stream.level)\n",
    "#          .count()\n",
    "#          .writeStream\n",
    "#          .outputMode('update')\n",
    "#          .format('console')\n",
    "#          .start())\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5240d",
   "metadata": {},
   "source": [
    "## Advanced Internals: Catalyst & Tungsten\n",
    "- **Catalyst**: Spark SQL optimizer that builds logical plan, applies rule-based and cost-based optimizations, and generates physical plan.\n",
    "- **Tungsten**: execution engine focusing on memory and CPU efficiency (off-heap, whole-stage code generation).\n",
    "\n",
    "**Keywords:** logical plan, physical plan, whole-stage codegen, vectorized execution, predicate pushdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886b0c4",
   "metadata": {},
   "source": [
    "## Performance Tuning & Best Practices\n",
    "- Use columnar formats (Parquet/ORC)\n",
    "- Avoid wide transformations when possible\n",
    "- Tune `spark.sql.shuffle.partitions`\n",
    "- Use broadcast joins for small lookup tables\n",
    "- Cache intermediate results only when reused\n",
    "- Use vectorized UDFs (pandas_udf) instead of row UDFs\n",
    "- Prefer built-in functions over Python UDFs\n",
    "\n",
    "**Keywords:** serialization (Kryo), memory fraction, executor cores, executor memory, GC tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: tuning shuffle partitions (conceptual)\n",
    "# spark.conf.set('spark.sql.shuffle.partitions', '200')\n",
    "# spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Example: enabling Kryo serialization\n",
    "# spark = SparkSession.builder.config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e5896",
   "metadata": {},
   "source": [
    "## MLlib (Machine Learning)\n",
    "**What this cell does:** introduces Spark MLlib API (DataFrame-based) and pipelines.\n",
    "**Internals:** algorithms adapted to distributed data; some are approximations (e.g., ALS for collaborative filtering).\n",
    "**Keywords:** Pipeline, Transformer, Estimator, fit, transform, MLlib, ALS, logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eee09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example MLlib pipeline (conceptual)\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "# assembler = VectorAssembler(inputCols=['feature1','feature2'], outputCol='features')\n",
    "# lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "# pipeline = Pipeline(stages=[assembler, lr])\n",
    "# model = pipeline.fit(train_df)\n",
    "# preds = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4f42a",
   "metadata": {},
   "source": [
    "## Debugging, Logging & UI\n",
    "- Use Spark Web UI (Driver URL) to inspect stages, tasks, executors, and storage.\n",
    "- Check driver/executor logs for stack traces.\n",
    "- Enable event logs for history server.\n",
    "\n",
    "**Keywords:** Spark UI, executor logs, eventLog.enabled, history server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: enabling event logs (conceptual)\n",
    "# spark = SparkSession.builder.config('spark.eventLog.enabled', 'true').config('spark.eventLog.dir','/tmp/spark-events').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3eaab9",
   "metadata": {},
   "source": [
    "## Local Pandas Examples for Quick Testing (useful when Spark not available locally)\n",
    "**What this cell does:** provides pandas equivalents for quick experimentation and mirrors PySpark examples so you can prototype logic locally before running on Spark cluster.\n",
    "**Internal notes:** Use pandas for small datasets; scale to Spark for big data.\n",
    "**Keywords:** pandas, prototype, sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134cd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sales = pd.read_csv('Pyspark_tutorial/pyspark_samples/sales.csv')\n",
    "users = pd.read_csv('Pyspark_tutorial/pyspark_samples/users.csv')\n",
    "logs = pd.read_csv('Pyspark_tutorial/pyspark_samples/logs.csv')\n",
    "\n",
    "# Quick analysis: total by customer\n",
    "print(sales.groupby('customer')['amount'].sum())\n",
    "# Join example\n",
    "print(sales.merge(users, left_on='order_id', right_on='user_id', how='left').head())\n",
    "# Time series: resample sales weekly sums\n",
    "sales['date'] = pd.to_datetime(sales['date'])\n",
    "print(sales.set_index('date').resample('7D')['amount'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9f77b",
   "metadata": {},
   "source": [
    "## Visual Diagrams\n",
    "Included diagrams: Big Data stack and Spark DAG/execution flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb94475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='Pyspark_tutorial/pyspark_samples/bigdata_stack.png'))\n",
    "display(Image(filename='Pyspark_tutorial/pyspark_samples/spark_dag.png'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
