{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea942c8f",
   "metadata": {},
   "source": [
    "# PySpark—Expanded Hands-on Notebook (Runnable + Fallback)\n",
    "This notebook attempts to create a local SparkSession and run small examples. If Spark/Java is not available\n",
    "in your environment, the notebook will gracefully fall back to using pandas for the same examples so you can\n",
    "develop logic locally and then run on a cluster.\n",
    "\n",
    "Files: large synthetic dataset (~200k rows) and a customer lookup CSV are included in `/samples/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a32c5d",
   "metadata": {},
   "source": [
    "## 0. Quick setup (install pyspark if needed)\n",
    "**What this cell does:** attempts to import pyspark; if missing, shows how to install it. Installation in some envs requires Java and may fail.\n",
    "**Notes:** If you have a cluster or local Spark installed, prefer to run the notebook with that Spark runtime.\n",
    "**Keywords:** pyspark, SparkSession, local[*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bde7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try importing pyspark; if not installed, instruct how to install.\n",
    "import importlib, sys\n",
    "pyspark_spec = importlib.util.find_spec('pyspark')\n",
    "if pyspark_spec is None:\n",
    "    print('pyspark not found in this environment. You can install with: pip install pyspark') \n",
    "    print('If you plan to run Spark locally, ensure Java (JDK 8/11) is installed and JAVA_HOME is set.')\n",
    "else:\n",
    "    import pyspark\n",
    "    print('pyspark available:', pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4cf6a",
   "metadata": {},
   "source": [
    "## 1. Create SparkSession (local mode) with safe fallback\n",
    "**What this cell does:** tries to create a SparkSession using local[*]. If it fails (no Java), falls back to a `None` spark variable and you can continue with pandas.\n",
    "**Notes on internals:** local[*] uses all CPU cores. SparkSession requires JVM; creating it will fail without Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e483c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession if possible; otherwise set spark = None and continue with pandas fallback\n",
    "spark = None\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.master('local[*]').appName('pyspark-notebook-local').getOrCreate()\n",
    "    print('SparkSession created:', spark)\n",
    "except Exception as e:\n",
    "    print('Could not create SparkSession:', e)\n",
    "    spark = None\n",
    "\n",
    "# Show available spark variable (None if not created)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4a00a",
   "metadata": {},
   "source": [
    "## 2. Read the large CSV (Spark if available; otherwise pandas)\n",
    "**What this cell does:** demonstrates reading the large CSV into Spark DataFrame (or pandas) and shows basic schema and counts.\n",
    "**Keywords:** spark.read.csv, inferSchema, header, toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b08f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "csv_path = r'/mnt/data/pyspark_expanded/samples/sales_200k.csv'\n",
    "lookup_path = r'/mnt/data/pyspark_expanded/samples/customers_lookup.csv'\n",
    "print('files exist?', os.path.exists(csv_path), os.path.exists(lookup_path))\n",
    "\n",
    "if spark is not None:\n",
    "    sdf = spark.read.option('header', True).option('inferSchema', True).csv(csv_path)\n",
    "    print('Spark schema:'); sdf.printSchema()\n",
    "    print('count (Spark):', sdf.count())\n",
    "    display(sdf.limit(5).toPandas())\n",
    "else:\n",
    "    import pandas as pd\n",
    "    pdf = pd.read_csv(csv_path, parse_dates=['event_ts'])\n",
    "    print('pandas df shape:', pdf.shape)\n",
    "    display(pdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af90962",
   "metadata": {},
   "source": [
    "## 3. Simple Transformations: filter, select, new column\n",
    "**What:** filter paid orders, compute VAT (10%), select columns. Demonstrates difference between lazy (Spark) and eager (pandas).\n",
    "**Notes:** In Spark, transformations are lazy until action (show/count/write) triggers execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93407786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "if spark is not None:\n",
    "    from pyspark.sql import functions as F\n",
    "    sdf2 = sdf.filter(F.col('is_paid') == True).withColumn('vat', F.col('amount') * 0.10).select('order_id','customer_id','amount','vat')\n",
    "    print('constructed transformations (lazy). Trigger action: show() -> executes job')\n",
    "    sdf2.show(5)\n",
    "    print('count:', sdf2.count())\n",
    "else:\n",
    "    pdf2 = pdf[pdf['is_paid'] == True].copy()\n",
    "    pdf2['vat'] = pdf2['amount'] * 0.10\n",
    "    print('pandas eager result shape:', pdf2.shape)\n",
    "    display(pdf2.head())\n",
    "print('elapsed:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d1afb",
   "metadata": {},
   "source": [
    "## 4. Aggregations & GroupBy (total amount per customer) with timing\n",
    "**What this cell does:** computes total amount per customer using Spark (groupBy) or pandas groupby and times both paths for profiling.\n",
    "**Keywords:** groupBy, agg, collect, toPandas, compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "t0 = time.time()\n",
    "if spark is not None:\n",
    "    from pyspark.sql import functions as F\n",
    "    res_spark = sdf.groupBy('customer_id').agg(F.count('*').alias('cnt'), F.sum('amount').alias('total_amount'))\n",
    "    # show top 5 by total_amount (triggering action)\n",
    "    top5 = res_spark.orderBy(F.col('total_amount').desc()).limit(5)\n",
    "    top5.show()\n",
    "    spark_time = time.time() - t0\n",
    "    print('Spark elapsed (s):', round(spark_time,3))\n",
    "else:\n",
    "    import pandas as pd\n",
    "    t0p = time.time()\n",
    "    res_pd = pdf.groupby('customer_id').agg(cnt=('order_id','count'), total_amount=('amount','sum')).reset_index()\n",
    "    res_pd = res_pd.sort_values('total_amount', ascending=False).head(5)\n",
    "    display(res_pd)\n",
    "    pandas_time = time.time() - t0p\n",
    "    print('pandas elapsed (s):', round(pandas_time,3))\n",
    "print('wall elapsed:', round(time.time()-t0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92096e7c",
   "metadata": {},
   "source": [
    "## 5. Join with lookup table (small broadcast join) and timing\n",
    "**What this cell does:** reads lookup CSV and joins to add customer metadata. For Spark it will demonstrate broadcast if the lookup is small.\n",
    "**Keywords:** broadcast, join, small lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "if spark is not None:\n",
    "    from pyspark.sql import functions as F\n",
    "    small = spark.read.option('header', True).option('inferSchema', True).csv(r'{lookup_csv}')\n",
    "    # force broadcast if small\n",
    "    from pyspark.sql.functions import broadcast\n",
    "    joined = sdf.join(broadcast(small), on='customer_id', how='left')\n",
    "    print('joined schema:'); joined.printSchema()\n",
    "    joined.show(5)\n",
    "    print('joined count:', joined.count())\n",
    "    print('spark join elapsed:', time.time()-t0)\n",
    "else:\n",
    "    import pandas as pd\n",
    "    small = pd.read_csv(r'{lookup_csv}')\n",
    "    joined = pdf.merge(small, on='customer_id', how='left')\n",
    "    display(joined.head())\n",
    "    print('pandas join elapsed:', time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81986157",
   "metadata": {},
   "source": [
    "## 6. Writing Results (Parquet) — with explanation\n",
    "**What this cell does:** writes a sample result to Parquet (if Spark available) or pandas to_parquet. Parquet is columnar and preserves schema.\n",
    "**Notes:** Spark write will create a folder with part files. pandas requires pyarrow or fastparquet to write parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r'{samples_dir}/out'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "if spark is not None:\n",
    "    # write partitioned on country if available, otherwise simple write\n",
    "    try:\n",
    "        sdf.limit(1000).write.mode('overwrite').parquet(out_dir + '/sample_parquet')\n",
    "        print('Wrote Parquet to', out_dir + '/sample_parquet')\n",
    "    except Exception as e:\n",
    "        print('Could not write parquet with Spark:', e)\n",
    "else:\n",
    "    try:\n",
    "        pdf.head(1000).to_parquet(out_dir + '/sample_parquet_pandas.parquet')\n",
    "        print('Wrote pandas parquet to', out_dir + '/sample_parquet_pandas.parquet')\n",
    "    except Exception as e:\n",
    "        print('Could not write parquet via pandas:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e55fb",
   "metadata": {},
   "source": [
    "## 7. Exercises — Beginner → Advanced (attempt these)\n",
    "Below are exercises you can run in this notebook. Solutions follow in the next cell.\n",
    "\n",
    "### Beginner\n",
    "1. Count number of paid orders.\n",
    "2. Find top 10 customers by total amount.\n",
    "\n",
    "### Intermediate\n",
    "3. For each day, compute number of orders and total amount.\n",
    "4. Compute rolling 7-day sum of amount for a specific customer (use pandas or Spark window functions).\n",
    "\n",
    "### Advanced\n",
    "5. Implement a UDF (pandas_udf if Spark available) to categorize orders into 'small','medium','large' by amount quantiles.\n",
    "6. Build a simple ML pipeline using Spark MLlib (e.g., predict `is_paid` from `amount` and `items`) — conceptual if Spark not available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33db12",
   "metadata": {},
   "source": [
    "## 8. Exercise Solutions (runnable)\n",
    "**What this cell does:** provides solutions for the exercises above. If Spark is available, uses Spark APIs; otherwise uses pandas equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions for exercises (will run for whichever engine is available)\n",
    "import time\n",
    "if spark is not None:\n",
    "    from pyspark.sql import functions as F\n",
    "    # 1. Count paid orders\n",
    "    c1 = sdf.filter(F.col('is_paid')==True).count()\n",
    "    print('1) paid orders count (spark):', c1)\n",
    "    # 2. top 10 customers by total amount\n",
    "    t2 = sdf.groupBy('customer_id').agg(F.sum('amount').alias('total')).orderBy(F.col('total').desc()).limit(10)\n",
    "    print('2) top 10 (spark):'); t2.show()\n",
    "    # 3. daily orders and totals (use date_trunc)\n",
    "    daily = sdf.withColumn('day', F.to_date('event_ts')).groupBy('day').agg(F.count('*').alias('orders'), F.sum('amount').alias('total'))\n",
    "    print('3) daily (spark):'); daily.show(5)\n",
    "    # 4. rolling 7-day per customer (window) -- demonstration for a sample customer\n",
    "    from pyspark.sql.window import Window\n",
    "    w = Window.partitionBy('customer_id').orderBy(F.col('event_ts').cast('long')).rangeBetween(-7*24*3600, 0)\n",
    "    roll = sdf.withColumn('rolling_7d', F.sum('amount').over(w)).filter(F.col('customer_id')== 'cust_1').select('event_ts','amount','rolling_7d').limit(5)\n",
    "    print('4) rolling window sample (spark):'); roll.show()\n",
    "    # 5. UDF categorization using pandas_udf (if available)\n",
    "    try:\n",
    "        from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "        import pandas as pd\n",
    "        @pandas_udf('string')\n",
    "        def quantile_cat(s: pd.Series) -> pd.Series:\n",
    "            q = s.quantile([0.33,0.66])\n",
    "            return pd.cut(s, bins=[-float('inf'), q.loc[0.33], q.loc[0.66], float('inf')], labels=['small','medium','large']).astype(str)\n",
    "        sample = sdf.select('amount').limit(1000).withColumn('cat', quantile_cat(F.col('amount')))\n",
    "        print('5) pandas_udf sample:'); sample.show(5)\n",
    "    except Exception as e:\n",
    "        print('pandas_udf not available or failed:', e)\n",
    "    # 6. MLlib conceptual demo (sketch)\n",
    "    print('6) MLlib pipeline sketch: use VectorAssembler -> LogisticRegression; fit on small sample. Run on cluster when available.')\n",
    "else:\n",
    "    # pandas solutions\n",
    "    import pandas as pd\n",
    "    pdf = pd.read_csv(r'{large_csv}', parse_dates=['event_ts'])\n",
    "    # 1\n",
    "    c1 = pdf[pdf['is_paid']==True].shape[0]\n",
    "    print('1) paid orders count (pandas):', c1)\n",
    "    # 2\n",
    "    t2 = pdf.groupby('customer_id')['amount'].sum().sort_values(ascending=False).head(10)\n",
    "    print('2) top 10 (pandas):'); display(t2)\n",
    "    # 3 daily\n",
    "    daily = pdf.set_index('event_ts').resample('D').agg({'order_id':'count','amount':'sum'}).rename(columns={'order_id':'orders'})\n",
    "    print('3) daily (pandas):'); display(daily.head())\n",
    "    # 4 rolling 7-day for cust_1\n",
    "    cust = pdf[pdf['customer_id']=='cust_1'].set_index('event_ts').sort_index()\n",
    "    cust['rolling_7d'] = cust['amount'].rolling('7D').sum()\n",
    "    print('4) rolling 7d (pandas) sample:'); display(cust.head())\n",
    "    # 5 categorize by quantiles\n",
    "    q = pdf['amount'].quantile([0.33,0.66])\n",
    "    pdf['cat'] = pd.cut(pdf['amount'], bins=[-float('inf'), q.loc[0.33], q.loc[0.66], float('inf')], labels=['small','medium','large'])\n",
    "    print('5) categories value counts:'); display(pdf['cat'].value_counts())\n",
    "    # 6 ML: sketch using scikit-learn locally (conceptual)\n",
    "    \n",
    "print('Solutions executed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae132fa7",
   "metadata": {},
   "source": [
    "## 9. Profiling cell (compare pandas timings for key ops)\n",
    "**What this cell does:** runs simple timing for read, groupby, join operations in pandas. If Spark available, also times Spark equivalents. This gives you an idea of wall-clock differences in your environment.\n",
    "**Keywords:** time, benchmark, profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timings = {}\n",
    "# pandas timings\n",
    "import pandas as pd\n",
    "t0 = time.time()\n",
    "pdf = pd.read_csv(r'{large_csv}', parse_dates=['event_ts'])\n",
    "timings['pandas_read_s'] = time.time() - t0\n",
    "t0 = time.time()\n",
    "res = pdf.groupby('customer_id')['amount'].sum()\n",
    "timings['pandas_groupby_s'] = time.time() - t0\n",
    "t0 = time.time()\n",
    "small = pd.read_csv(r'{lookup_csv}')\n",
    "joined = pdf.merge(small, on='customer_id', how='left')\n",
    "timings['pandas_join_s'] = time.time() - t0\n",
    "\n",
    "# spark timings (if available)\n",
    "if spark is not None:\n",
    "    from pyspark.sql import functions as F\n",
    "    t0 = time.time()\n",
    "    sdf_local = spark.read.option('header',True).option('inferSchema',True).csv(r'{large_csv}')\n",
    "    timings['spark_read_s'] = time.time() - t0\n",
    "    t0 = time.time()\n",
    "    g = sdf_local.groupBy('customer_id').agg(F.sum('amount'))\n",
    "    g.count()  # trigger\n",
    "    timings['spark_groupby_s'] = time.time() - t0\n",
    "    t0 = time.time()\n",
    "    small_s = spark.read.option('header',True).option('inferSchema',True).csv(r'{lookup_csv}')\n",
    "    from pyspark.sql.functions import broadcast\n",
    "    j = sdf_local.join(broadcast(small_s), on='customer_id', how='left')\n",
    "    j.count()\n",
    "    timings['spark_join_s'] = time.time() - t0\n",
    "\n",
    "print('Timings (seconds):', timings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791aacce",
   "metadata": {},
   "source": [
    "## 10. Markdown Cheat-sheet (separate file included in ZIP)\n",
    "I also generated a concise markdown cheat-sheet included in the ZIP as `pyspark_cheatsheet.md`. It summarizes commands, common functions, and tuning tips."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
