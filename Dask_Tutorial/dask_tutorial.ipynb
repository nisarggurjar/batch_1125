{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86afcb84",
   "metadata": {},
   "source": [
    "# Dask: Beginner → Advanced Tutorial\n",
    "\n",
    "A hands-on Jupyter notebook covering core Dask topics from basics to advanced. Includes examples, notes, and practice exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47107d6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- Basic knowledge of pandas and NumPy\n",
    "\n",
    "**Optional** installs (run if you need them):\n",
    "\n",
    "```bash\n",
    "pip install dask[complete] distributed --upgrade\n",
    "pip install matplotlib pandas nbformat\n",
    "```\n",
    "\n",
    "*(The install cell below is commented — run it in your local environment if needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f1738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask[complete] distributed matplotlib pandas\n",
    "# Uncomment and run the line above in your environment if Dask isn't installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4bb8b",
   "metadata": {},
   "source": [
    "## Quick imports & version check\n",
    "*Check installed versions of dask and distributed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, distributed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('dask version:', dask.__version__)\n",
    "print('distributed version:', distributed.__version__)\n",
    "print('pandas version:', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6118186",
   "metadata": {},
   "source": [
    "## Create sample CSV files\n",
    "We'll create several small CSV files to demonstrate `dd.read_csv` with glob patterns and partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os, numpy as np\n",
    "os.makedirs('sample_csvs', exist_ok=True)\n",
    "n_files = 4\n",
    "rows_per = 250\n",
    "for i in range(n_files):\n",
    "    df = pd.DataFrame({\n",
    "        'id': range(i*rows_per, (i+1)*rows_per),\n",
    "        'group': np.random.choice(['A','B','C'], size=rows_per),\n",
    "        'value': np.random.randn(rows_per) * 100 + 500,\n",
    "        'date': pd.date_range('2020-01-01', periods=rows_per).astype(str)\n",
    "    })\n",
    "    df.to_csv(f'sample_csvs/data_part_{i}.csv', index=False)\n",
    "print('Created', n_files, 'CSV files in ./sample_csvs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9c703",
   "metadata": {},
   "source": [
    "## Read multiple CSVs as a Dask DataFrame\n",
    "Use `dd.read_csv` with a glob pattern. Dask will create partitions automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "ddf = dd.read_csv('sample_csvs/data_part_*.csv', parse_dates=['date'])\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9576128c",
   "metadata": {},
   "source": [
    "## Basic operations & lazy evaluation\n",
    "Dask operations are lazy — they build a task graph. Use `.head()`, `.compute()`, `.persist()` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1357c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataframe (lazy)\n",
    "print('npartitions =', ddf.npartitions)\n",
    "print('columns =', ddf.columns.tolist())\n",
    "\n",
    "# Compute a small sample\n",
    "print('\\n.head() sample:')\n",
    "display(ddf.head())\n",
    "\n",
    "# Perform a groupby aggregation (lazy until compute)\n",
    "agg = ddf.groupby('group').value.mean()\n",
    "print('\\nAggregation object (lazy):', type(agg))\n",
    "\n",
    "# Compute result\n",
    "print('\\nComputed result:')\n",
    "display(agg.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ea1ce",
   "metadata": {},
   "source": [
    "## Count NULL/NA values per column\n",
    "Example to count nulls (and empty strings) in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab7daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls per column (Dask DataFrame approach)\n",
    "null_counts = ddf.isnull().sum().compute()\n",
    "print('Null counts per column:') \n",
    "print(null_counts)\n",
    "\n",
    "# Count null+empty strings for object columns (compute needed)\n",
    "def count_null_or_empty(col):\n",
    "    return ((ddf[col].isnull()) | (ddf[col] == '')).sum()\n",
    "\n",
    "# Example for 'group' column\n",
    "print('\\nNull or empty in \"group\":', count_null_or_empty('group').compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8cf2b",
   "metadata": {},
   "source": [
    "## Fill NULL values with mean (numeric columns)\n",
    "Compute mean and use `fillna` with the dictionary of means. Note: computing means requires communication and `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15421af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of numeric columns\n",
    "numeric_cols = [c for c, t in ddf.dtypes.items() if str(t).startswith(('int','float'))]\n",
    "\n",
    "# Compute means for numeric columns\n",
    "means = ddf[numeric_cols].mean().compute().to_dict()\n",
    "print('Means:', means)\n",
    "\n",
    "# Fill nulls using the means dict (returns a new ddf)\n",
    "ddf_filled = ddf.fillna(means)\n",
    "print(ddf_filled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b1405",
   "metadata": {},
   "source": [
    "## Dask Delayed - convert Python functions into lazy tasks\n",
    "`delayed` is useful for wrapping arbitrary Python functions and building custom DAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05805749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import time\n",
    "\n",
    "@delayed\n",
    "def slow_square(x):\n",
    "    time.sleep(0.1)\n",
    "    return x*x\n",
    "\n",
    "tasks = [slow_square(i) for i in range(10)]\n",
    "sum_task = delayed(sum)(tasks)\n",
    "res = sum_task.compute()  # runs tasks in parallel with the local scheduler\n",
    "print('sum of squares:', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1020ad",
   "metadata": {},
   "source": [
    "## Dask Futures (distributed) example\n",
    "Use `distributed.Client` for a scheduler and submit tasks as futures. Run this cell only if you can start a local client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "# client = Client(n_workers=2, threads_per_worker=2)  # uncomment to start a scheduler\n",
    "# print(client)\n",
    "# future = client.submit(lambda x: x + 1, 10)\n",
    "# print('future result:', future.result())\n",
    "print('This cell is a template: start a Client locally or on a cluster to use Futures.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754053d",
   "metadata": {},
   "source": [
    "## Visualize Task Graphs\n",
    "Use `.visualize()` on Dask collections (requires graphviz if rendering inline). Example below shows intent — rendering depends on your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfbe10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the aggregation graph (saves to file if graphviz available)\n",
    "try:\n",
    "    agg = ddf.groupby('group').value.mean()\n",
    "    agg.visualize(filename='agg_graph.png')  # requires graphviz in the environment\n",
    "    print('Saved agg_graph.png (if graphviz installed).')\n",
    "except Exception as e:\n",
    "    print('Graphviz not available or visualization failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045715f",
   "metadata": {},
   "source": [
    "## Repartitioning & performance tips\n",
    "\n",
    "- Aim for partition sizes ~100–250 MB.\n",
    "- Use `.repartition(npartitions=...)` or `.repartition(partition_size='200MB')`.\n",
    "- Use `.persist()` when reusing intermediate results.\n",
    "- Avoid wide shuffles unless necessary (joins, groupbys with high-cardinality keys).\n",
    "- Prefer Parquet for storage.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example repartition\n",
    "print('Before repartition:', ddf.npartitions)\n",
    "ddf2 = ddf.repartition(npartitions=2)\n",
    "print('After repartition:', ddf2.npartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec72c68",
   "metadata": {},
   "source": [
    "## Dask for ML & Integration (notes)\n",
    "\n",
    "- Use `dask-ml` for scaling scikit-learn workflows.\n",
    "- Use Dask with XGBoost for distributed training.\n",
    "- For hyperparameter tuning, use `dask-ml`'s `IncrementalSearchCV` or tools such as Optuna with Dask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21cd6f",
   "metadata": {},
   "source": [
    "## Persisting results & writing to Parquet\n",
    "Write results using `.to_parquet()` for efficient storage and later reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2afbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to parquet (local)\n",
    "out_dir = 'dask_out_parquet'\n",
    "ddf.to_parquet(out_dir, engine='pyarrow', overwrite=True)\n",
    "print('Wrote parquet to', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e6c0e",
   "metadata": {},
   "source": [
    "## Exercises & Practice Tasks\n",
    "\n",
    "1. Read a large dataset using `dd.read_csv` and compute mean by a category.\n",
    "2. Use `delayed` to parallelize a pure-Python data ingestion function.\n",
    "3. Repartition data by a datetime column and run a rolling-window aggregation.\n",
    "4. Measure performance before/after adding `.persist()`.\n",
    "\n",
    "Try implementing these and use the Dask dashboard to inspect workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46639f",
   "metadata": {},
   "source": [
    "## Troubleshooting & Resources\n",
    "\n",
    "- Official docs: https://docs.dask.org/\n",
    "- Dask tutorial: https://tutorial.dask.org/\n",
    "- Dask on distributed: https://distributed.dask.org/\n",
    "\n",
    "If you hit memory errors: reduce partitions, increase workers, or use spill-to-disk options."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
