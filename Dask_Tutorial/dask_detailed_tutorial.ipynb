{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88744e14",
   "metadata": {},
   "source": [
    "# Dask — Complete Notes & Roadmap (Beginner → Advanced)\n",
    "\n",
    "This notebook is a structured, practical guide to Dask used by data engineers and ML teams. Follow the sections step-by-step. Each section contains short notes, examples, and exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf58d18",
   "metadata": {},
   "source": [
    "## Setup & Installation\n",
    "\n",
    "Install Dask and distributed (recommended):\n",
    "\n",
    "```bash\n",
    "pip install \"dask[complete]\" distributed --upgrade\n",
    "# Optional: for dask-ml, xgboost, and cloud storage\n",
    "pip install dask-ml xgboost s3fs gcsfs adlfs\n",
    "```\n",
    "\n",
    "Start a `Client()` in examples below to enable the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02a755",
   "metadata": {},
   "source": [
    "## Beginner — Fundamentals\n",
    "\n",
    "### What is Dask?\n",
    "- Dask is a flexible parallel computing library for analytics.\n",
    "- It scales Python workloads from a laptop to a cluster by parallelizing existing Python libraries (NumPy, Pandas, Scikit-learn).\n",
    "\n",
    "### When to use Dask vs Pandas / Spark\n",
    "- Use **Pandas**: small data that fits in memory and single-machine workflows.\n",
    "- Use **Dask**: when you want to scale existing pandas/NumPy code to out-of-core or multiple cores/machines with minimal changes.\n",
    "- Use **Spark**: if you need JVM ecosystem integrations, very large clusters, or mature production orchestration in some orgs.\n",
    "\n",
    "### Dask ecosystem\n",
    "- **dask.dataframe (dd)** — pandas-like API for tabular data.\n",
    "- **dask.array (da)** — NumPy-like arrays.\n",
    "- **dask.bag** — for unstructured or semi-structured data.\n",
    "- **dask.delayed** — turn arbitrary Python functions into lazy tasks.\n",
    "- **dask.distributed** — scheduler & cluster management (Client, workers, dashboard).\n",
    "- **dask-ml** — scalable ML utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and starting a local Client (distributed scheduler)\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "# Start a local client. In many environments, simply calling Client() will suffice.\n",
    "client = Client()  # opens a local cluster and exposes a dashboard link in many environments\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebaf34",
   "metadata": {},
   "source": [
    "### Dask DataFrame — Basics\n",
    "- Dask DataFrame mirrors the pandas API but operations are **lazy**.\n",
    "- Data is partitioned into many pandas DataFrames, each processed in parallel.\n",
    "- Use `.compute()` to get results, `.persist()` to cache intermediate results in memory.\n",
    "\n",
    "**Key differences**: no `.iloc` for mixed-position indexing across partitions, some pandas operations may not be supported or may be expensive (shuffles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6228a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multiple CSV files (the sample CSVs created alongside this notebook)\n",
    "import dask.dataframe as dd\n",
    "sample_path = \"Dask_Tutorial/dask_tutorial_with_data/dask_sample_data/sample_part_*.csv\"\n",
    "ddf = dd.read_csv(sample_path, parse_dates=['timestamp'])\n",
    "ddf.head()  # triggers a small computation to fetch first partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic operations: selection, filter, groupby, aggregations\n",
    "# Note: these return lazy Dask objects until compute() is called.\n",
    "df = ddf\n",
    "print('npartitions:', df.npartitions)\n",
    "\n",
    "# Select columns\n",
    "sel = df[['id', 'timestamp', 'value']]\n",
    "\n",
    "# Filter rows\n",
    "filtered = df[df['flag'] == 1]\n",
    "\n",
    "# Groupby and aggregation (lazy)\n",
    "agg = df.groupby('category').value.mean()\n",
    "\n",
    "# Compute results\n",
    "print('Aggregations result:')\n",
    "print(agg.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the task graph for an operation (requires graphviz installed to render locally)\n",
    "agg.visualize(filename='dask_agg_graph')  # saves a .png/svg in working directory if graphviz available\n",
    "print('Saved a visualization file if graphviz is present.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae56c32",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "- Use `.isna()`, `.dropna()`, `.fillna()` — same API as pandas.\n",
    "- Repartitioning before heavy operations can help (but avoid unnecessary shuffles).\n",
    "\n",
    "**Tip**: For large datasets, prefer operations that act partition-wise to avoid costly shuffles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e19bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: fillna and dropna (partition-wise)\n",
    "# Create a column with some missing values for demo\n",
    "df2 = ddf.assign(v2=ddf['value'])\n",
    "df2['v2'] = df2['v2'].mask(df2['v2'] < df2['v2'].quantile(0.05), None)\n",
    "res = df2['v2'].fillna(df2['v2'].mean())\n",
    "print('Computed mean-filled sample:')\n",
    "print(res.head().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51856960",
   "metadata": {},
   "source": [
    "### Saving Output\n",
    "- Write to Parquet is recommended (columnar, faster reads, metadata preserved).\n",
    "\n",
    "```python\n",
    "ddf.to_parquet('out/parquet_dataset')\n",
    "# or write partitioned by a column\n",
    "ddf.to_parquet('out/parquet_dataset', partition_on=['category'])\n",
    "```\n",
    "\n",
    "Parquet is typically faster and recommended for downstream processing or cloud storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d392b5",
   "metadata": {},
   "source": [
    "## Intermediate — Performance & Scaling\n",
    "\n",
    "### Partitions\n",
    "- Check partitions: `ddf.npartitions`.\n",
    "- Repartition: `ddf.repartition(npartitions=...)` or `ddf.repartition(divisions=...)`.\n",
    "- Best partition size: ~100–250 MB (varies by workload and memory).\n",
    "\n",
    "### Task Graphs\n",
    "- Dask builds a DAG of tasks; operations are lazy until `.compute()`.\n",
    "- `.visualize()` lets you inspect the DAG; large graphs can be simplified via `optimize_graph=True` in some functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: repartition and checking sizes\n",
    "print('Original npartitions:', ddf.npartitions)\n",
    "small = ddf.repartition(npartitions=3)\n",
    "print('New npartitions:', small.npartitions)\n",
    "small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d681f",
   "metadata": {},
   "source": [
    "### Dask Delayed\n",
    "- Use `dask.delayed` to parallelize arbitrary Python functions.\n",
    "- Build custom DAGs and compute or persist only when needed.\n",
    "\n",
    "### Dask Futures\n",
    "- With `client.submit` and `client.map`, you get futures for real-time parallelism and control.\n",
    "- Useful for asynchronous workflows and non-data-parallel tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36021ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: dask.delayed\n",
    "from dask import delayed\n",
    "import time\n",
    "\n",
    "@delayed\n",
    "def slow_double(x):\n",
    "    time.sleep(0.2)\n",
    "    return x * 2\n",
    "\n",
    "tasks = [slow_double(i) for i in range(10)]\n",
    "res = delayed(sum)(tasks)\n",
    "print(res.compute())  # triggers parallel execution using the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d911c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: futures with client.submit\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "futures = client.map(add, range(5), range(5,10))\n",
    "results = client.gather(futures)\n",
    "print('Futures results:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bdc48",
   "metadata": {},
   "source": [
    "### Optimization Techniques\n",
    "- `persist()` caches intermediate results on workers — good for re-use.\n",
    "- Use `compute()` on final results; avoid calling `.compute()` inside loops or prematurely.\n",
    "- Minimize shuffles (joins, groupbys across partitions).\n",
    "- Use categorical columns where possible to reduce memory.\n",
    "- Monitor the dashboard for memory and task-time hotspots.\n",
    "\n",
    "**Example:** `df = df.categorize(columns=['category'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeba364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist example\n",
    "cached = ddf.persist()\n",
    "print('Persisted dataframe with npartitions:', cached.npartitions)\n",
    "# small computation\n",
    "print(cached.head().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f140828",
   "metadata": {},
   "source": [
    "## Advanced — Production\n",
    "\n",
    "### Distributed scheduler internals\n",
    "- Work stealing, task fusion, and resource restrictions help with efficient scheduling.\n",
    "- You can set worker resources and constraints (e.g., `resources={'GPU':1}`).\n",
    "\n",
    "### Dask on Kubernetes / Cloud\n",
    "- Use DaskKubernetes or helm charts to deploy clusters and autoscaling.\n",
    "- Integrate with S3/GCS using `s3fs` and `gcsfs`.\n",
    "\n",
    "### Dask Array\n",
    "- Similar API to NumPy. Use chunking and rechunking for efficient computations.\n",
    "\n",
    "### Dask Bag\n",
    "- For unstructured/line-based data (logs, JSON), with `bag.map`, `bag.filter`.\n",
    "\n",
    "### Time series with Dask\n",
    "- Partition by time and use `set_index('timestamp')` carefully (may require shuffle).\n",
    "- Rolling and resampling operate per-partition; ensure contiguous time ranges when needed.\n",
    "\n",
    "### Dask for ML\n",
    "- dask-ml provides parallelized hyperparameter search and scalable estimators.\n",
    "- Many scikit-learn estimators can be parallelized using joblib or dask-ml wrappers.\n",
    "\n",
    "### XGBoost + Dask\n",
    "- XGBoost supports dask interface for distributed training across workers (CPU/GPU).\n",
    "\n",
    "### Deploying pipelines\n",
    "- Orchestrate with Airflow/Prefect, monitor with Prometheus/Grafana, and handle retries for idempotency.\n",
    "\n",
    "### Performance debugging\n",
    "- Use `client.profile()`, worker logs, and `client.run_on_scheduler` for introspection.\n",
    "- Avoid large Python objects in the graph; prefer DataFrames/arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747d1ca",
   "metadata": {},
   "source": [
    "## Expert (Optional)\n",
    "- Custom schedulers, CUDA workers with RAPIDS, writing custom blockwise ops, and low-level graph optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises & Practice\n",
    "1. Read the sample CSV files using `dd.read_csv` and compute the mean `value` per `category`.\n",
    "2. Convert `category` to categorical and measure memory improvement.\n",
    "3. Create an expensive Python function and parallelize it using `dask.delayed` and `client.map`. Compare runtimes.\n",
    "4. Write out a filtered subset to Parquet partitioned by `category`.\n",
    "\n",
    "Try to run these exercises and inspect the dashboard while they run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
