{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f044bb",
   "metadata": {},
   "source": [
    "# Polars — Comprehensive Tutorial (Beginner → Advanced)\n",
    "This notebook contains explanations, notes, internal workings, keywords, and runnable examples.\n",
    "Each code cell includes short comments explaining what it does and notes on internals where relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c1dae",
   "metadata": {},
   "source": [
    "## Setup: install and import\n",
    "**What this cell does:** installs polars and imports libraries.\n",
    "**Internal notes:** `polars` core is written in Rust and exposes bindings — installation pulls the wheel.\n",
    "**Keywords:** eager, lazy, expressions, scan_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d545b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install polars if not present (uncomment if needed)\n",
    "# !pip install polars\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('polars version:', pl.__version__)\n",
    "# Create a tiny DF to check\n",
    "print(pl.DataFrame({'a':[1,2,3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473303b",
   "metadata": {},
   "source": [
    "## Creating DataFrames (Eager)\n",
    "**What this cell does:** shows different ways to create DataFrames.\n",
    "**Internal notes:** Eager `pl.DataFrame` materializes immediately in memory (Arrow memory layout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python dict (eager)\n",
    "df = pl.DataFrame({'name': ['Alice','Bob'], 'age':[25,30]})\n",
    "print(df)\n",
    "\n",
    "# From pandas DataFrame\n",
    "pdf = pd.DataFrame({'x':[1,2,3], 'y':[4,5,6]})\n",
    "df2 = pl.from_pandas(pdf)\n",
    "print(df2)\n",
    "\n",
    "# From CSV (eager read)\n",
    "df_csv = pl.read_csv('/mnt/data/polars_samples/sales.csv')\n",
    "print('read_csv rows:', df_csv.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31d362",
   "metadata": {},
   "source": [
    "## Expressions & Selecting\n",
    "**What this cell does:** demonstrates `pl.col`, `pl.expr` and using expressions.\n",
    "**Internal notes:** Expressions are lazily built computation graphs — when used in eager context are evaluated immediately; in lazy context they become part of a query plan.\n",
    "**Keywords:** `pl.col`, `pl.lit`, `.select()`, `.with_columns()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f68de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use expressions to transform columns\n",
    "print(df_csv.select([\n",
    "    pl.col('customer'),\n",
    "    (pl.col('amount') * 1.1).alias('amount_with_tax')\n",
    "]))\n",
    "\n",
    "# Add derived column with with_columns\n",
    "print(df_csv.with_columns((pl.col('amount')/pl.col('amount').max()).alias('amount_norm')).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebcbd6",
   "metadata": {},
   "source": [
    "## Filter, GroupBy & Aggregation\n",
    "**What this cell does:** basic filtering and aggregation.\n",
    "**Internals:** GroupBy triggers parallel aggregation using multiple threads and Arrow buffers.\n",
    "**Keywords:** `.filter()`, `.groupby()`, `.agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_csv.filter(pl.col('amount') > 50).head())\n",
    "print(df_csv.groupby('customer').agg([pl.col('amount').sum().alias('total')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd804f5",
   "metadata": {},
   "source": [
    "## Joins\n",
    "**What this cell does:** demonstrates join types and notes on performance.\n",
    "**Internals:** Polars uses hash joins by default for equality joins; joins can be expensive if keys are large.\n",
    "**Keywords:** `.join()`, how='inner|left|outer|cross'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e349a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pl.DataFrame({'id':[1,2,3], 'v':[10,20,30]})\n",
    "right = pl.DataFrame({'id':[2,3,4], 'w':[200,300,400]})\n",
    "print(left.join(right, on='id', how='inner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117fec4",
   "metadata": {},
   "source": [
    "## Lazy Mode & scan_* vs read_*()\n",
    "**What this cell does:** shows lazy API using `scan_csv` and explains difference.\n",
    "**Internals:** `scan_csv` builds a lazy plan without loading data; `read_csv` loads immediately. Lazy plans allow predicate pushdown, projection pushdown, and query optimization.\n",
    "**Keywords:** `scan_csv`, `.lazy()`, `.collect()`, predicate pushdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36359a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.scan_csv('/mnt/data/polars_samples/sales.csv')  # lazy scan, no IO executed yet\n",
    "print(type(lf))\n",
    "\n",
    "# Build a lazy pipeline: filter then aggregate — executed on collect()\n",
    "plan = lf.filter(pl.col('amount') > 50).groupby('customer').agg(pl.col('amount').sum())\n",
    "print('lazy plan repr:')\n",
    "print(plan)\n",
    "print('collect() executes plan and returns eager DataFrame:')\n",
    "print(plan.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a9646",
   "metadata": {},
   "source": [
    "## Predicate Pushdown, Projection Pushdown & Query Optimization\n",
    "**What this cell does:** demonstrates how placing filters before projections can reduce IO in lazy mode.\n",
    "**Internals:** Optimizer rewrites query DAG, moves filters earlier (predicate pushdown), and removes unused columns (projection pushdown).\n",
    "**Keywords:** optimizer, dag, pushdown, physical plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: compare applying filter early vs late in lazy pipeline\n",
    "lf = pl.scan_csv('/mnt/data/polars_samples/sales.csv')\n",
    "# Filter early (good)\n",
    "plan_early = lf.filter(pl.col('amount') > 50).select(['customer','amount']).collect()\n",
    "# Filter late (suboptimal but optimizer often rewrites)\n",
    "plan_late = lf.select(['customer','amount']).filter(pl.col('amount') > 50).collect()\n",
    "print(plan_early)\n",
    "print(plan_late)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859c0d2",
   "metadata": {},
   "source": [
    "## Fold (Expression-level Reduce)\n",
    "**What this cell does:** demonstrates `fold` for iterative expression aggregation.\n",
    "**Internals:** `fold` reduces a list of expressions into a single expression (useful for variable-length columns or many columns).\n",
    "**Keywords:** `pl.fold`, `acc`, `function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: sum many columns via fold\n",
    "df_multi = pl.DataFrame({'a':[1,2], 'b':[3,4], 'c':[5,6]})\n",
    "exprs = [pl.col(c) for c in df_multi.columns]\n",
    "sum_expr = pl.fold(acc=pl.lit(0), function=lambda a, b: a + b, exprs=exprs).alias('total')\n",
    "print(df_multi.select(sum_expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd07b8",
   "metadata": {},
   "source": [
    "## HStack & VStack (Concatenation)\n",
    "**What this cell does:** stack DataFrames horizontally and vertically.\n",
    "**Internals:** hstack reuses memory where possible for speed; vstack concatenates row-wise and may reallocate.\n",
    "**Keywords:** `.hstack()`, `.vstack()`, `pl.concat()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pl.DataFrame({'a':[1,2]})\n",
    "df_b = pl.DataFrame({'b':[3,4]})\n",
    "print(df_a.hstack(df_b))\n",
    "print(pl.concat([df_a, df_a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec253db5",
   "metadata": {},
   "source": [
    "## Struct & List columns (Nested types)\n",
    "**What this cell does:** shows creating and manipulating nested columns.\n",
    "**Internals:** Nested Arrow types store offsets for lists and struct fields; operations on them are optimized but can be more costly to serialize.\n",
    "**Keywords:** `pl.Struct`, `pl.List`, `.explode()`, `.arr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fabb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create struct and list columns\n",
    "df_nested = pl.DataFrame({'id':[1,2], 'lst':[[1,2],[3]], 'meta':[{'a':1},{'b':2}]})\n",
    "print(df_nested)\n",
    "# Explode list column\n",
    "print(df_nested.explode('lst'))\n",
    "# Access struct-like keys by dot access if it's a struct\n",
    "struct_df = df_nested.select([pl.struct(['id','lst']).alias('s')])\n",
    "print(struct_df.select(pl.col('s').arr.eval(pl.element().sum()).alias('struct_demo'), pl.col('s')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d342b",
   "metadata": {},
   "source": [
    "## UDFs & Performance\n",
    "**What this cell does:** explains using `apply` vs vectorized expressions and performance implications.\n",
    "**Internals:** `.apply()` runs Python-level function per-row and is slower; prefer built-in expressions which are executed in Rust and parallelized.\n",
    "**Keywords:** `apply`, `map`, `expr`, vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf = pl.DataFrame({'x': list(range(1000))})\n",
    "# Slow: python apply\n",
    "import time\n",
    "start = time.time()\n",
    "res = df_perf.with_columns(pl.col('x').apply(lambda v: v*2).alias('x2'))\n",
    "print('apply time (approx):', time.time()-start)\n",
    "# Fast: vectorized expression (Rust)\n",
    "start = time.time()\n",
    "res2 = df_perf.with_columns((pl.col('x')*2).alias('x2'))\n",
    "print('expr time (approx):', time.time()-start)\n",
    "print(res2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda37937",
   "metadata": {},
   "source": [
    "## Streaming & Scanning Many Files\n",
    "**What this cell does:** shows `scan_csv` and `streaming=True` for memory constrained pipelines.\n",
    "**Internals:** Streaming mode processes data in chunks and avoids fully materializing intermediate results.\n",
    "**Keywords:** `streaming=True`, `scan_csv`, memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration we use scan_csv; streaming is most useful when collecting to file-level sinks.\n",
    "lf = pl.scan_csv('\" + sales_path + \"')\n",
    "res = lf.filter(pl.col('amount') > 50).select(['date','amount']).collect(streaming=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9520a",
   "metadata": {},
   "source": [
    "## Performance Tips (Practical)\n",
    "- Prefer expressions over `apply`.\n",
    "- Use lazy + scan for large files.\n",
    "- Use predicate & projection pushdown.\n",
    "- Repartition if one partition is too large (not often needed).\n",
    "- Use arrow IPC or parquet for faster IO.\n",
    "\n",
    "**Keywords:** probe, predicate pushdown, mutex, threads, rayon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tip: read parquet (if available) is faster than CSV\n",
    "# pl.read_parquet('file.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d358a2",
   "metadata": {},
   "source": [
    "## Visual Diagrams\n",
    "Below are two example diagrams included in the package: sales timeseries and Polars architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff790ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='/mnt/data/polars_samples/sales_timeseries.png'))\n",
    "display(Image(filename='/mnt/data/polars_samples/polars_architecture.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc431f9",
   "metadata": {},
   "source": [
    "## Reading Newline-delimited JSON (example: nested types)\n",
    "**What this cell does:** reads the sample nested JSON created earlier.\n",
    "**Keywords:** `read_ndjson`, `read_json`, nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592514a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.read_ndjson('/mnt/data/polars_samples/nested.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da178872",
   "metadata": {},
   "source": [
    "## Writing Files (CSV, Parquet)\n",
    "**What this cell does:** demonstrates writing results.\n",
    "**Internals:** parquet preserves schema and nested types whereas CSV flattens and may lose types.\n",
    "**Keywords:** `.write_parquet()`, `.write_csv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pl.read_csv('/mnt/data/polars_samples/sales.csv')\n",
    "df_csv.write_csv('/mnt/data/polars_samples/out_sales.csv')\n",
    "# For parquet (example)\n",
    "# df_csv.write_parquet('/mnt/data/polars_samples/out_sales.parquet')\n",
    "print('wrote out_sales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2a3dd",
   "metadata": {},
   "source": [
    "## Closing Notes & Further Reading\n",
    "- Polars is ideal for heavy-data tasks when you need speed.\n",
    "- Use the official docs and API reference for the latest specialized APIs.\n",
    "\n",
    "**End of notebook.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
